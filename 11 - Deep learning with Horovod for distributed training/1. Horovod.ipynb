{"cells":[{"cell_type":"markdown","source":["# Horovod\n\nHorovodRunner is a general API to run distributed DL workloads on Databricks using Uber’s [Horovod](https://github.com/uber/horovod) framework. By integrating Horovod with Spark’s barrier mode, Databricks is able to provide higher stability for long-running deep learning training jobs on Spark. HorovodRunner takes a Python method that contains DL training code with Horovod hooks. This method gets pickled on the driver and sent to Spark workers. A Horovod MPI job is embedded as a Spark job using barrier execution mode. The first executor collects the IP addresses of all task executors using BarrierTaskContext and triggers a Horovod job using mpirun. Each Python MPI process loads the pickled program back, deserializes it, and runs it.\n\n<br>\n\n![](https://files.training.databricks.com/images/horovod-runner.png)\n\nFor additional resources, see:\n* [Horovod Runner Docs](https://docs.microsoft.com/en-us/azure/databricks/applications/deep-learning/distributed-training/horovod-runner)\n* [Horovod Runner webinar](https://vimeo.com/316872704/e79235f62c)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"775a56f4-2239-468c-aa21-881906036d0e"}}},{"cell_type":"markdown","source":["Run the following cell to set up our environment."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66cda9af-f842-4786-aa94-ed9b232a1ee4"}}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c47df75-89f6-4ae5-b729-0af95e0f98ee"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"The execution of this command did not finish successfully","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Build Model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3cb5903-266e-4e46-aa4a-775fa08fb437"}}},{"cell_type":"code","source":["import numpy as np\nnp.random.seed(0)\nimport tensorflow as tf\n# tf.set_random_seed(42) # For reproducibility\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\ndef build_model():\n  return Sequential([Dense(20, input_dim=8, activation='relu'),\n                    Dense(20, activation='relu'),\n                    Dense(1, activation='linear')]) # Keep the output layer as linear because this is a regression problem"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7e3fae6f-a139-4968-b43d-63fd4bcbe4fb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Shard Data\n\nFrom the [Horovod docs](https://github.com/horovod/horovod/blob/master/docs/concepts.rst):\n\nHorovod core principles are based on the MPI concepts size, rank, local rank, allreduce, allgather, and broadcast. These are best explained by example. Say we launched a training script on 4 servers, each having 4 GPUs. If we launched one copy of the script per GPU:\n\n* Size would be the number of processes, in this case, 16.\n\n* Rank would be the unique process ID from 0 to 15 (size - 1).\n\n* Local rank would be the unique process ID within the server from 0 to 3.\n\nWe need to shard our data across our processes.  **NOTE:** We are using a Pandas DataFrame for demo purposes. In the next notebook we will use Parquet files with Petastorm for better scalability."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7c31c95-eea4-4b5c-a32e-bae0b2c84f96"}}},{"cell_type":"code","source":["from sklearn.datasets.california_housing import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\ndef get_dataset(rank=0, size=1):\n  scaler = StandardScaler()\n  cal_housing = fetch_california_housing(data_home=\"/dbfs/ml/\" + str(rank) + \"/\")\n  X_train, X_test, y_train, y_test = train_test_split(cal_housing.data,\n                                                       cal_housing.target,\n                                                       test_size=0.2,\n                                                       random_state=1)\n  scaler.fit(X_train)\n  X_train = scaler.transform(X_train[rank::size])\n  y_train = y_train[rank::size]\n  X_test = scaler.transform(X_test[rank::size])\n  y_test = y_test[rank::size]\n  return (X_train, y_train), (X_test, y_test)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f958472b-4ba5-4dc3-b3ed-84bd1926b2c6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Horovod"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"847b7777-0482-4bea-8d54-c20d2cda6cec"}}},{"cell_type":"code","source":["from tensorflow.keras import optimizers\nimport horovod.tensorflow.keras as hvd\nfrom tensorflow.keras import backend as K\n\ndef run_training_horovod():\n  # Horovod: initialize Horovod.\n  hvd.init()\n  # If using GPU: pin GPU to be used to process local rank (one GPU per process)\n  # config = tf.ConfigProto()\n  # config.gpu_options.allow_growth = True\n  # config.gpu_options.visible_device_list = str(hvd.local_rank())\n  # K.set_session(tf.Session(config=config))\n  print(f\"Rank is: {hvd.rank()}\")\n  print(f\"Size is: {hvd.size()}\")\n  \n  (X_train, y_train), (X_test, y_test) = get_dataset(hvd.rank(), hvd.size())\n  \n  model = build_model()\n  \n  from tensorflow.keras import optimizers\n  # Horovod: adjust learning rate based on number of GPUs/CPUs.\n  optimizer = optimizers.Adam(lr=0.001*hvd.size())\n  \n  # Horovod: add Horovod Distributed Optimizer.\n  optimizer = hvd.DistributedOptimizer(optimizer)\n\n  model.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mse\"])\n  \n  history = model.fit(X_train, y_train, validation_split=.2, epochs=10, batch_size=64, verbose=2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cedc96bb-2bf8-4805-a9da-06f73bf56533"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Test it out on just the driver."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d05f35e6-6a72-4ee3-8e59-44015f0999cd"}}},{"cell_type":"code","source":["from sparkdl import HorovodRunner\nhr = HorovodRunner(np=-1)\nhr.run(run_training_horovod)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"440ef8f1-7b65-4199-ae89-d24e5d480c9f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">HorovodRunner will stream all training logs to notebook cell output. If there are too many logs, you\ncan adjust the log level in your train method. Or you can set driver_log_verbosity to\n&#39;log_callback_only&#39; and use a HorovodRunner log  callback on the first worker to get concise\nprogress updates.\nHorovodRunner will launch Horovod jobs on the driver node. There would be resource contention if you\nshare the cluster with others.\nThe global names read or written to by the pickled function are {&#39;print&#39;, &#39;hvd&#39;, &#39;get_dataset&#39;, &#39;build_model&#39;}.\nThe pickled object size is 2831 bytes.\n\n### How to enable Horovod Timeline? ###\nHorovodRunner has the ability to record the timeline of its activity with Horovod  Timeline. To\nrecord a Horovod Timeline, set the `HOROVOD_TIMELINE` environment variable  to the location of the\ntimeline file to be created. You can then open the timeline file  using the chrome://tracing\nfacility of the Chrome browser.\n\nExecuting command: [&#39;mpirun&#39;, &#39;--allow-run-as-root&#39;, &#39;-np&#39;, &#39;1&#39;, &#39;-H&#39;, &#39;localhost&#39;, &#39;--stdin&#39;, &#39;none&#39;, &#39;--tag-output&#39;, &#39;-mca&#39;, &#39;rmaps&#39;, &#39;seq&#39;, &#39;--bind-to&#39;, &#39;none&#39;, &#39;-x&#39;, &#39;NCCL_DEBUG=INFO&#39;, &#39;-mca&#39;, &#39;pml&#39;, &#39;ob1&#39;, &#39;-mca&#39;, &#39;btl&#39;, &#39;^openib&#39;, &#39;-mca&#39;, &#39;plm_rsh_agent&#39;, &#39;ssh -o StrictHostKeyChecking=no -i /tmp/HorovodRunner_5d75ed17755a/id_rsa&#39;, &#39;bash&#39;, &#39;/tmp/HorovodRunner_5d75ed17755a/launch.sh&#39;].\n\n[1,0]&lt;stdout&gt;:Rank is: 0\n[1,0]&lt;stdout&gt;:Size is: 1\n[1,0]&lt;stderr&gt;:Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /dbfs/ml/0/\n[1,0]&lt;stderr&gt;:Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /dbfs/ml/0/\n[1,0]&lt;stderr&gt;:2022-03-09 09:36:18.184488: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n[1,0]&lt;stderr&gt;:To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n[1,0]&lt;stderr&gt;:2022-03-09 09:36:18.202582: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2095074999 Hz\n[1,0]&lt;stderr&gt;:2022-03-09 09:36:18.203033: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x564cab93dfa0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n[1,0]&lt;stderr&gt;:2022-03-09 09:36:18.203086: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n[1,0]&lt;stdout&gt;:Epoch 1/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 3.4254 - mse: 3.4254 - val_loss: 1.0853 - val_mse: 1.0853\n[1,0]&lt;stdout&gt;:Epoch 2/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.8421 - mse: 0.8421 - val_loss: 0.7221 - val_mse: 0.7221\n[1,0]&lt;stdout&gt;:Epoch 3/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.6505 - mse: 0.6505 - val_loss: 0.5928 - val_mse: 0.5928\n[1,0]&lt;stdout&gt;:Epoch 4/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.5393 - mse: 0.5393 - val_loss: 0.5145 - val_mse: 0.5145\n[1,0]&lt;stdout&gt;:Epoch 5/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.4727 - mse: 0.4727 - val_loss: 0.4708 - val_mse: 0.4708\n[1,0]&lt;stdout&gt;:Epoch 6/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.4391 - mse: 0.4391 - val_loss: 0.4530 - val_mse: 0.4530\n[1,0]&lt;stdout&gt;:Epoch 7/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.4171 - mse: 0.4171 - val_loss: 0.4228 - val_mse: 0.4228\n[1,0]&lt;stdout&gt;:Epoch 8/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.3990 - mse: 0.3990 - val_loss: 0.4111 - val_mse: 0.4111\n[1,0]&lt;stdout&gt;:Epoch 9/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.3844 - mse: 0.3844 - val_loss: 0.3953 - val_mse: 0.3953\n[1,0]&lt;stdout&gt;:Epoch 10/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.3798 - mse: 0.3798 - val_loss: 0.3875 - val_mse: 0.3875\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">HorovodRunner will stream all training logs to notebook cell output. If there are too many logs, you\ncan adjust the log level in your train method. Or you can set driver_log_verbosity to\n&#39;log_callback_only&#39; and use a HorovodRunner log  callback on the first worker to get concise\nprogress updates.\nHorovodRunner will launch Horovod jobs on the driver node. There would be resource contention if you\nshare the cluster with others.\nThe global names read or written to by the pickled function are {&#39;print&#39;, &#39;hvd&#39;, &#39;get_dataset&#39;, &#39;build_model&#39;}.\nThe pickled object size is 2831 bytes.\n\n### How to enable Horovod Timeline? ###\nHorovodRunner has the ability to record the timeline of its activity with Horovod  Timeline. To\nrecord a Horovod Timeline, set the `HOROVOD_TIMELINE` environment variable  to the location of the\ntimeline file to be created. You can then open the timeline file  using the chrome://tracing\nfacility of the Chrome browser.\n\nExecuting command: [&#39;mpirun&#39;, &#39;--allow-run-as-root&#39;, &#39;-np&#39;, &#39;1&#39;, &#39;-H&#39;, &#39;localhost&#39;, &#39;--stdin&#39;, &#39;none&#39;, &#39;--tag-output&#39;, &#39;-mca&#39;, &#39;rmaps&#39;, &#39;seq&#39;, &#39;--bind-to&#39;, &#39;none&#39;, &#39;-x&#39;, &#39;NCCL_DEBUG=INFO&#39;, &#39;-mca&#39;, &#39;pml&#39;, &#39;ob1&#39;, &#39;-mca&#39;, &#39;btl&#39;, &#39;^openib&#39;, &#39;-mca&#39;, &#39;plm_rsh_agent&#39;, &#39;ssh -o StrictHostKeyChecking=no -i /tmp/HorovodRunner_5d75ed17755a/id_rsa&#39;, &#39;bash&#39;, &#39;/tmp/HorovodRunner_5d75ed17755a/launch.sh&#39;].\n\n[1,0]&lt;stdout&gt;:Rank is: 0\n[1,0]&lt;stdout&gt;:Size is: 1\n[1,0]&lt;stderr&gt;:Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /dbfs/ml/0/\n[1,0]&lt;stderr&gt;:Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /dbfs/ml/0/\n[1,0]&lt;stderr&gt;:2022-03-09 09:36:18.184488: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n[1,0]&lt;stderr&gt;:To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n[1,0]&lt;stderr&gt;:2022-03-09 09:36:18.202582: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2095074999 Hz\n[1,0]&lt;stderr&gt;:2022-03-09 09:36:18.203033: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x564cab93dfa0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n[1,0]&lt;stderr&gt;:2022-03-09 09:36:18.203086: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n[1,0]&lt;stdout&gt;:Epoch 1/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 3.4254 - mse: 3.4254 - val_loss: 1.0853 - val_mse: 1.0853\n[1,0]&lt;stdout&gt;:Epoch 2/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.8421 - mse: 0.8421 - val_loss: 0.7221 - val_mse: 0.7221\n[1,0]&lt;stdout&gt;:Epoch 3/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.6505 - mse: 0.6505 - val_loss: 0.5928 - val_mse: 0.5928\n[1,0]&lt;stdout&gt;:Epoch 4/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.5393 - mse: 0.5393 - val_loss: 0.5145 - val_mse: 0.5145\n[1,0]&lt;stdout&gt;:Epoch 5/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.4727 - mse: 0.4727 - val_loss: 0.4708 - val_mse: 0.4708\n[1,0]&lt;stdout&gt;:Epoch 6/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.4391 - mse: 0.4391 - val_loss: 0.4530 - val_mse: 0.4530\n[1,0]&lt;stdout&gt;:Epoch 7/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.4171 - mse: 0.4171 - val_loss: 0.4228 - val_mse: 0.4228\n[1,0]&lt;stdout&gt;:Epoch 8/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.3990 - mse: 0.3990 - val_loss: 0.4111 - val_mse: 0.4111\n[1,0]&lt;stdout&gt;:Epoch 9/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.3844 - mse: 0.3844 - val_loss: 0.3953 - val_mse: 0.3953\n[1,0]&lt;stdout&gt;:Epoch 10/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.3798 - mse: 0.3798 - val_loss: 0.3875 - val_mse: 0.3875\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Better Horovod"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"73a8be20-c71c-4aa0-8631-ce634d5d85ab"}}},{"cell_type":"code","source":["from tensorflow.keras import optimizers\nfrom tensorflow.keras.callbacks import *\n\ndef run_training_horovod():\n  # Horovod: initialize Horovod.\n  hvd.init()\n  # If using GPU: pin GPU to be used to process local rank (one GPU per process)\n  # config = tf.ConfigProto()\n  # config.gpu_options.allow_growth = True\n  # config.gpu_options.visible_device_list = str(hvd.local_rank())\n  # K.set_session(tf.Session(config=config))\n  \n  \n  \n  print(f\"Rank is: {hvd.rank()}\")\n  print(f\"Size is: {hvd.size()}\")\n  \n  (X_train, y_train), (X_test, y_test) = get_dataset(hvd.rank(), hvd.size())\n  \n  model = build_model()\n  \n  from tensorflow.keras import optimizers\n  # Horovod: adjust learning rate based on number of GPUs.\n  optimizer = optimizers.Adam(lr=0.001*hvd.size())\n  \n  # Horovod: add Horovod Distributed Optimizer.\n  optimizer = hvd.DistributedOptimizer(optimizer)\n\n  model.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mse\"])\n  ml_working_path = \"pinky.gtm@mail.kmutt.ac.th\"\n  # Use the optimized FUSE Mount\n  checkpoint_dir = f\"{ml_working_path}/horovod_checkpoint_weights.ckpt\"\n  \n  callbacks = [\n    # Horovod: broadcast initial variable states from rank 0 to all other processes.\n    # This is necessary to ensure consistent initialization of all workers when\n    # training is started with random weights or restored from a checkpoint.\n    hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n\n    # Horovod: average metrics among workers at the end of every epoch.\n    # Note: This callback must be in the list before the ReduceLROnPlateau,\n    # TensorBoard or other metrics-based callbacks.\n    hvd.callbacks.MetricAverageCallback(),\n\n    # Horovod: using `lr = 1.0 * hvd.size()` from the very beginning leads to worse final\n    # accuracy. Scale the learning rate `lr = 1.0` ---> `lr = 1.0 * hvd.size()` during\n    # the first five epochs. See https://arxiv.org/abs/1706.02677 for details.\n    hvd.callbacks.LearningRateWarmupCallback(warmup_epochs=5, verbose=1),\n    \n    # Reduce the learning rate if training plateaus.\n    ReduceLROnPlateau(patience=10, verbose=1)\n  ]\n  \n  # Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\n  if hvd.rank() == 0:\n    callbacks.append(ModelCheckpoint(checkpoint_dir, save_weights_only=True))\n  \n  history = model.fit(X_train, y_train, validation_split=.2, epochs=10, batch_size=64, verbose=2, callbacks=callbacks)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"51c3e36d-9f9f-480d-93d1-fd5b6d8ba0fe"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Test it out on just the driver."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bcd9f180-0530-4eb1-b773-441c1f3708c3"}}},{"cell_type":"code","source":["from sparkdl import HorovodRunner\nhr = HorovodRunner(np=-1)\nhr.run(run_training_horovod)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be4a9170-da6b-45b6-a09c-073210930087"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">HorovodRunner will stream all training logs to notebook cell output. If there are too many logs, you\ncan adjust the log level in your train method. Or you can set driver_log_verbosity to\n&#39;log_callback_only&#39; and use a HorovodRunner log  callback on the first worker to get concise\nprogress updates.\nHorovodRunner will launch Horovod jobs on the driver node. There would be resource contention if you\nshare the cluster with others.\nThe global names read or written to by the pickled function are {&#39;ModelCheckpoint&#39;, &#39;hvd&#39;, &#39;build_model&#39;, &#39;ReduceLROnPlateau&#39;, &#39;print&#39;, &#39;get_dataset&#39;}.\nThe pickled object size is 3348 bytes.\n\n### How to enable Horovod Timeline? ###\nHorovodRunner has the ability to record the timeline of its activity with Horovod  Timeline. To\nrecord a Horovod Timeline, set the `HOROVOD_TIMELINE` environment variable  to the location of the\ntimeline file to be created. You can then open the timeline file  using the chrome://tracing\nfacility of the Chrome browser.\n\nExecuting command: [&#39;mpirun&#39;, &#39;--allow-run-as-root&#39;, &#39;-np&#39;, &#39;1&#39;, &#39;-H&#39;, &#39;localhost&#39;, &#39;--stdin&#39;, &#39;none&#39;, &#39;--tag-output&#39;, &#39;-mca&#39;, &#39;rmaps&#39;, &#39;seq&#39;, &#39;--bind-to&#39;, &#39;none&#39;, &#39;-x&#39;, &#39;NCCL_DEBUG=INFO&#39;, &#39;-mca&#39;, &#39;pml&#39;, &#39;ob1&#39;, &#39;-mca&#39;, &#39;btl&#39;, &#39;^openib&#39;, &#39;-mca&#39;, &#39;plm_rsh_agent&#39;, &#39;ssh -o StrictHostKeyChecking=no -i /tmp/HorovodRunner_bbe999b468f4/id_rsa&#39;, &#39;bash&#39;, &#39;/tmp/HorovodRunner_bbe999b468f4/launch.sh&#39;].\n\n[1,0]&lt;stdout&gt;:Rank is: 0\n[1,0]&lt;stdout&gt;:Size is: 1\n[1,0]&lt;stderr&gt;:2022-03-09 09:40:53.167713: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n[1,0]&lt;stderr&gt;:To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n[1,0]&lt;stderr&gt;:2022-03-09 09:40:53.173692: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2095074999 Hz\n[1,0]&lt;stderr&gt;:2022-03-09 09:40:53.174133: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5557872a3310 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n[1,0]&lt;stderr&gt;:2022-03-09 09:40:53.174183: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n[1,0]&lt;stdout&gt;:Epoch 1/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 1.4516 - mse: 1.4516 - val_loss: 0.6863 - val_mse: 0.6863\n[1,0]&lt;stdout&gt;:Epoch 2/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.6057 - mse: 0.6057 - val_loss: 0.5098 - val_mse: 0.5098\n[1,0]&lt;stdout&gt;:Epoch 3/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.4758 - mse: 0.4758 - val_loss: 0.4532 - val_mse: 0.4532\n[1,0]&lt;stdout&gt;:Epoch 4/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.4273 - mse: 0.4273 - val_loss: 0.4240 - val_mse: 0.4240\n[1,0]&lt;stdout&gt;:Epoch 5/10\n[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:Epoch 5: finished gradual learning rate warmup to 0.001.\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.4077 - mse: 0.4077 - val_loss: 0.4181 - val_mse: 0.4181\n[1,0]&lt;stdout&gt;:Epoch 6/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.3955 - mse: 0.3955 - val_loss: 0.4034 - val_mse: 0.4034\n[1,0]&lt;stdout&gt;:Epoch 7/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.3874 - mse: 0.3874 - val_loss: 0.3987 - val_mse: 0.3987\n[1,0]&lt;stdout&gt;:Epoch 8/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.3806 - mse: 0.3806 - val_loss: 0.3942 - val_mse: 0.3942\n[1,0]&lt;stdout&gt;:Epoch 9/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.3748 - mse: 0.3748 - val_loss: 0.3882 - val_mse: 0.3882\n[1,0]&lt;stdout&gt;:Epoch 10/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.3694 - mse: 0.3694 - val_loss: 0.3836 - val_mse: 0.3836\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">HorovodRunner will stream all training logs to notebook cell output. If there are too many logs, you\ncan adjust the log level in your train method. Or you can set driver_log_verbosity to\n&#39;log_callback_only&#39; and use a HorovodRunner log  callback on the first worker to get concise\nprogress updates.\nHorovodRunner will launch Horovod jobs on the driver node. There would be resource contention if you\nshare the cluster with others.\nThe global names read or written to by the pickled function are {&#39;ModelCheckpoint&#39;, &#39;hvd&#39;, &#39;build_model&#39;, &#39;ReduceLROnPlateau&#39;, &#39;print&#39;, &#39;get_dataset&#39;}.\nThe pickled object size is 3348 bytes.\n\n### How to enable Horovod Timeline? ###\nHorovodRunner has the ability to record the timeline of its activity with Horovod  Timeline. To\nrecord a Horovod Timeline, set the `HOROVOD_TIMELINE` environment variable  to the location of the\ntimeline file to be created. You can then open the timeline file  using the chrome://tracing\nfacility of the Chrome browser.\n\nExecuting command: [&#39;mpirun&#39;, &#39;--allow-run-as-root&#39;, &#39;-np&#39;, &#39;1&#39;, &#39;-H&#39;, &#39;localhost&#39;, &#39;--stdin&#39;, &#39;none&#39;, &#39;--tag-output&#39;, &#39;-mca&#39;, &#39;rmaps&#39;, &#39;seq&#39;, &#39;--bind-to&#39;, &#39;none&#39;, &#39;-x&#39;, &#39;NCCL_DEBUG=INFO&#39;, &#39;-mca&#39;, &#39;pml&#39;, &#39;ob1&#39;, &#39;-mca&#39;, &#39;btl&#39;, &#39;^openib&#39;, &#39;-mca&#39;, &#39;plm_rsh_agent&#39;, &#39;ssh -o StrictHostKeyChecking=no -i /tmp/HorovodRunner_bbe999b468f4/id_rsa&#39;, &#39;bash&#39;, &#39;/tmp/HorovodRunner_bbe999b468f4/launch.sh&#39;].\n\n[1,0]&lt;stdout&gt;:Rank is: 0\n[1,0]&lt;stdout&gt;:Size is: 1\n[1,0]&lt;stderr&gt;:2022-03-09 09:40:53.167713: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n[1,0]&lt;stderr&gt;:To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n[1,0]&lt;stderr&gt;:2022-03-09 09:40:53.173692: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2095074999 Hz\n[1,0]&lt;stderr&gt;:2022-03-09 09:40:53.174133: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5557872a3310 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n[1,0]&lt;stderr&gt;:2022-03-09 09:40:53.174183: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n[1,0]&lt;stdout&gt;:Epoch 1/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 1.4516 - mse: 1.4516 - val_loss: 0.6863 - val_mse: 0.6863\n[1,0]&lt;stdout&gt;:Epoch 2/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.6057 - mse: 0.6057 - val_loss: 0.5098 - val_mse: 0.5098\n[1,0]&lt;stdout&gt;:Epoch 3/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.4758 - mse: 0.4758 - val_loss: 0.4532 - val_mse: 0.4532\n[1,0]&lt;stdout&gt;:Epoch 4/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.4273 - mse: 0.4273 - val_loss: 0.4240 - val_mse: 0.4240\n[1,0]&lt;stdout&gt;:Epoch 5/10\n[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:Epoch 5: finished gradual learning rate warmup to 0.001.\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.4077 - mse: 0.4077 - val_loss: 0.4181 - val_mse: 0.4181\n[1,0]&lt;stdout&gt;:Epoch 6/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.3955 - mse: 0.3955 - val_loss: 0.4034 - val_mse: 0.4034\n[1,0]&lt;stdout&gt;:Epoch 7/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.3874 - mse: 0.3874 - val_loss: 0.3987 - val_mse: 0.3987\n[1,0]&lt;stdout&gt;:Epoch 8/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.3806 - mse: 0.3806 - val_loss: 0.3942 - val_mse: 0.3942\n[1,0]&lt;stdout&gt;:Epoch 9/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.3748 - mse: 0.3748 - val_loss: 0.3882 - val_mse: 0.3882\n[1,0]&lt;stdout&gt;:Epoch 10/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.3694 - mse: 0.3694 - val_loss: 0.3836 - val_mse: 0.3836\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Run on all workers"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f8f0dd9-698b-4cd0-9b4e-0f261db8937c"}}},{"cell_type":"code","source":["from sparkdl import HorovodRunner\nhr = HorovodRunner(np=0)\nhr.run(run_training_horovod)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82393b05-faa7-4dba-8e80-2ae384ca843a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">HorovodRunner will stream all training logs to notebook cell output. If there are too many logs, you\ncan adjust the log level in your train method. Or you can set driver_log_verbosity to\n&#39;log_callback_only&#39; and use a HorovodRunner log  callback on the first worker to get concise\nprogress updates.\nSetting np=0 is deprecated and it will be removed in the next major Databricks Runtime release.\nChoosing np based on the total task slots at runtime is unreliable due to dynamic executor\nregistration. Please set the number of parallel processes you need explicitly.\nThe global names read or written to by the pickled function are {&#39;ModelCheckpoint&#39;, &#39;hvd&#39;, &#39;build_model&#39;, &#39;ReduceLROnPlateau&#39;, &#39;print&#39;, &#39;get_dataset&#39;}.\nThe pickled object size is 3350 bytes.\n\n### How to enable Horovod Timeline? ###\nHorovodRunner has the ability to record the timeline of its activity with Horovod  Timeline. To\nrecord a Horovod Timeline, set the `HOROVOD_TIMELINE` environment variable  to the location of the\ntimeline file to be created. You can then open the timeline file  using the chrome://tracing\nfacility of the Chrome browser.\n\nWARNING:root:Waiting for executor registration before HorovodRunner could start. 0 seconds / 120 seconds\nWARNING:root:Waiting for executor registration before HorovodRunner could start. 5 seconds / 120 seconds\nWARNING:root:Waiting for executor registration before HorovodRunner could start. 10 seconds / 120 seconds\nWARNING:root:Waiting for executor registration before HorovodRunner could start. 15 seconds / 120 seconds\nWARNING:root:Waiting for executor registration before HorovodRunner could start. 20 seconds / 120 seconds\nWARNING:root:Waiting for executor registration before HorovodRunner could start. 25 seconds / 120 seconds\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">HorovodRunner will stream all training logs to notebook cell output. If there are too many logs, you\ncan adjust the log level in your train method. Or you can set driver_log_verbosity to\n&#39;log_callback_only&#39; and use a HorovodRunner log  callback on the first worker to get concise\nprogress updates.\nSetting np=0 is deprecated and it will be removed in the next major Databricks Runtime release.\nChoosing np based on the total task slots at runtime is unreliable due to dynamic executor\nregistration. Please set the number of parallel processes you need explicitly.\nThe global names read or written to by the pickled function are {&#39;ModelCheckpoint&#39;, &#39;hvd&#39;, &#39;build_model&#39;, &#39;ReduceLROnPlateau&#39;, &#39;print&#39;, &#39;get_dataset&#39;}.\nThe pickled object size is 3350 bytes.\n\n### How to enable Horovod Timeline? ###\nHorovodRunner has the ability to record the timeline of its activity with Horovod  Timeline. To\nrecord a Horovod Timeline, set the `HOROVOD_TIMELINE` environment variable  to the location of the\ntimeline file to be created. You can then open the timeline file  using the chrome://tracing\nfacility of the Chrome browser.\n\nWARNING:root:Waiting for executor registration before HorovodRunner could start. 0 seconds / 120 seconds\nWARNING:root:Waiting for executor registration before HorovodRunner could start. 5 seconds / 120 seconds\nWARNING:root:Waiting for executor registration before HorovodRunner could start. 10 seconds / 120 seconds\nWARNING:root:Waiting for executor registration before HorovodRunner could start. 15 seconds / 120 seconds\nWARNING:root:Waiting for executor registration before HorovodRunner could start. 20 seconds / 120 seconds\nWARNING:root:Waiting for executor registration before HorovodRunner could start. 25 seconds / 120 seconds\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"1. Horovod","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1546012998597247}},"nbformat":4,"nbformat_minor":0}
