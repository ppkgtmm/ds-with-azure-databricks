{"cells":[{"cell_type":"markdown","source":["# Model Selection\n\nBuilding machine learning solutions involves testing a number of different models.  This lesson explores tuning hyperparameters and cross-validation in order to select the optimal model as well as saving models and predictions.\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n* Define hyperparameters and motivate their role in machine learning\n* Tune hyperparameters using grid search\n* Validate model performance using cross-validation\n* Save a trained model and its predictions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b9e889a-8b58-42df-b279-0f999ddef881"}}},{"cell_type":"markdown","source":["<iframe  \nsrc=\"//fast.wistia.net/embed/iframe/vaq1zoh9k4?videoFoam=true\"\nstyle=\"border:1px solid #1cb1c2;\"\nallowtransparency=\"true\" scrolling=\"no\" class=\"wistia_embed\"\nname=\"wistia_embed\" allowfullscreen mozallowfullscreen webkitallowfullscreen\noallowfullscreen msallowfullscreen width=\"640\" height=\"360\" ></iframe>\n<div>\n<a target=\"_blank\" href=\"https://fast.wistia.net/embed/iframe/vaq1zoh9k4?seo=false\">\n  <img alt=\"Opens in new tab\" src=\"https://files.training.databricks.com/static/images/external-link-icon-16x16.png\"/>&nbsp;Watch full-screen.</a>\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fa75ab8f-4462-4163-b8ee-fd93bbf33b69"}}},{"cell_type":"markdown","source":["-sandbox\n### Tuning, Validating and Saving\n\nIn earlier lessons, we addressed the methodological mistake of training _and_ evaluating a model on the same data.  This leads to **overfitting,** where the model performs well on data it has already seen but fails to predict anything useful on data it has not already seen.  To solve this, we proposed the train/test split where we divided our dataset between a training set used to train the model and a test set used to evaluate the model's performance on unseen data.  In this lesson, we will explore a more rigorous solution to problem of overfitting.\n\nA **hyperparameter** is a parameter used in a machine learning algorithm that is set before the learning process begins.  In other words, a machine learning algorithm cannot learn hyperparameters from the data itself.  Hyperparameters need to be tested and validated by training multiple models.  Common hyperparameters include the number of iterations and the complexity of the model.  **Hyperparameter tuning** is the process of choosing the hyperparameter that performs the best on our loss function, or the way we penalize an algorithm for being wrong.\n\nIf we were to train a number of different models with different hyperparameters and then evaluate their performance on the test set, we would still risk overfitting because we might choose the hyperparameter that just so happens to perform the best on the data we have in our dataset.  To solve this, we can use _k_ subsets of our training set to train our model, a process called **_k_-fold cross-validation.** \n\n<div><img src=\"https://files.training.databricks.com/images/eLearning/ML-Part-1/cross-validation.png\" style=\"height: 400px; margin: 20px\"/></div>\n\nIn this lesson, we will divide our dataset into _k_ \"folds\" in order to choose the best hyperparameters for our machine learning model.  We will then save the trained model and its predictions."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96bf1b1c-5013-4f73-8779-c0abd99c9cb8"}}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d6e0143e-4cf7-482b-b351-b41d94215e1b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"The execution of this command did not finish successfully","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n### Hyperparameter Tuning\n\nHyperparameter tuning is the process of of choosing the optimal hyperparameters for a machine learning algorithm.  Each algorithm has different hyperparameters to tune.  You can explore these hyperparameters by using the `.explainParams()` method on a model.\n\n**Grid search** is the process of exhaustively trying every combination of hyperparameters.  It takes all of the values we want to test and combines them in every possible way so that we test them using cross-validation.\n\nStart by performing a train/test split on the Boston dataset and building a pipeline for linear regression.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> See <a href=\"https://en.wikipedia.org/wiki/Hyperparameter_optimization\" target=\"_blank\">the Wikipedia article on hyperparameter optimization</a> for more information."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b3ba5c9-053f-49cf-b7ce-69c40826ead0"}}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\nbostonDF = (spark.read\n  .option(\"HEADER\", True)\n  .option(\"inferSchema\", True)\n  .csv(\"/mnt/training/bostonhousing/bostonhousing/bostonhousing.csv\")\n  .drop(\"_c0\")\n)\n\ntrainDF, testDF = bostonDF.randomSplit([0.8, 0.2], seed=42)\n\nassembler = VectorAssembler(inputCols=bostonDF.columns[:-1], outputCol=\"features\")\n\nlr = (LinearRegression()\n  .setLabelCol(\"medv\")\n  .setFeaturesCol(\"features\")\n)\n\npipeline = Pipeline(stages = [assembler, lr])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e362c5f4-2053-49bc-9224-3aa086056030"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"bostonDF","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"crim","nullable":true,"type":"double"},{"metadata":{},"name":"zn","nullable":true,"type":"double"},{"metadata":{},"name":"indus","nullable":true,"type":"double"},{"metadata":{},"name":"chas","nullable":true,"type":"integer"},{"metadata":{},"name":"nox","nullable":true,"type":"double"},{"metadata":{},"name":"rm","nullable":true,"type":"double"},{"metadata":{},"name":"age","nullable":true,"type":"double"},{"metadata":{},"name":"dis","nullable":true,"type":"double"},{"metadata":{},"name":"rad","nullable":true,"type":"integer"},{"metadata":{},"name":"tax","nullable":true,"type":"integer"},{"metadata":{},"name":"ptratio","nullable":true,"type":"double"},{"metadata":{},"name":"black","nullable":true,"type":"double"},{"metadata":{},"name":"lstat","nullable":true,"type":"double"},{"metadata":{},"name":"medv","nullable":true,"type":"double"}],"type":"struct"},"tableIdentifier":null},{"name":"trainDF","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"crim","nullable":true,"type":"double"},{"metadata":{},"name":"zn","nullable":true,"type":"double"},{"metadata":{},"name":"indus","nullable":true,"type":"double"},{"metadata":{},"name":"chas","nullable":true,"type":"integer"},{"metadata":{},"name":"nox","nullable":true,"type":"double"},{"metadata":{},"name":"rm","nullable":true,"type":"double"},{"metadata":{},"name":"age","nullable":true,"type":"double"},{"metadata":{},"name":"dis","nullable":true,"type":"double"},{"metadata":{},"name":"rad","nullable":true,"type":"integer"},{"metadata":{},"name":"tax","nullable":true,"type":"integer"},{"metadata":{},"name":"ptratio","nullable":true,"type":"double"},{"metadata":{},"name":"black","nullable":true,"type":"double"},{"metadata":{},"name":"lstat","nullable":true,"type":"double"},{"metadata":{},"name":"medv","nullable":true,"type":"double"}],"type":"struct"},"tableIdentifier":null},{"name":"testDF","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"crim","nullable":true,"type":"double"},{"metadata":{},"name":"zn","nullable":true,"type":"double"},{"metadata":{},"name":"indus","nullable":true,"type":"double"},{"metadata":{},"name":"chas","nullable":true,"type":"integer"},{"metadata":{},"name":"nox","nullable":true,"type":"double"},{"metadata":{},"name":"rm","nullable":true,"type":"double"},{"metadata":{},"name":"age","nullable":true,"type":"double"},{"metadata":{},"name":"dis","nullable":true,"type":"double"},{"metadata":{},"name":"rad","nullable":true,"type":"integer"},{"metadata":{},"name":"tax","nullable":true,"type":"integer"},{"metadata":{},"name":"ptratio","nullable":true,"type":"double"},{"metadata":{},"name":"black","nullable":true,"type":"double"},{"metadata":{},"name":"lstat","nullable":true,"type":"double"},{"metadata":{},"name":"medv","nullable":true,"type":"double"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Take a look at the model parameters using the `.explainParams()` method."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f072af25-7056-4a62-86ac-8e99f9c46826"}}},{"cell_type":"code","source":["print(lr.explainParams())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"45dc95aa-310d-44db-9056-74257935f470"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">aggregationDepth: suggested depth for treeAggregate (&gt;= 2). (default: 2)\nelasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\nepsilon: The shape parameter to control the amount of robustness. Must be &gt; 1.0. Only valid when loss is huber (default: 1.35)\nfeaturesCol: features column name. (default: features, current: features)\nfitIntercept: whether to fit an intercept term. (default: True)\nlabelCol: label column name. (default: label, current: medv)\nloss: The loss function to be optimized. Supported options: squaredError, huber. (default: squaredError)\nmaxIter: max number of iterations (&gt;= 0). (default: 100)\npredictionCol: prediction column name. (default: prediction)\nregParam: regularization parameter (&gt;= 0). (default: 0.0)\nsolver: The solver algorithm for optimization. Supported options: auto, normal, l-bfgs. (default: auto)\nstandardization: whether to standardize the training features before fitting the model. (default: True)\ntol: the convergence tolerance for iterative algorithms (&gt;= 0). (default: 1e-06)\nweightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">aggregationDepth: suggested depth for treeAggregate (&gt;= 2). (default: 2)\nelasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\nepsilon: The shape parameter to control the amount of robustness. Must be &gt; 1.0. Only valid when loss is huber (default: 1.35)\nfeaturesCol: features column name. (default: features, current: features)\nfitIntercept: whether to fit an intercept term. (default: True)\nlabelCol: label column name. (default: label, current: medv)\nloss: The loss function to be optimized. Supported options: squaredError, huber. (default: squaredError)\nmaxIter: max number of iterations (&gt;= 0). (default: 100)\npredictionCol: prediction column name. (default: prediction)\nregParam: regularization parameter (&gt;= 0). (default: 0.0)\nsolver: The solver algorithm for optimization. Supported options: auto, normal, l-bfgs. (default: auto)\nstandardization: whether to standardize the training features before fitting the model. (default: True)\ntol: the convergence tolerance for iterative algorithms (&gt;= 0). (default: 1e-06)\nweightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n`ParamGridBuilder()` allows us to string together all of the different possible hyperparameters we would like to test.  In this case, we can test the maximum number of iterations, whether we want to use an intercept with the y axis, and whether we want to standardize our features.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> Since grid search works through exhaustively building a model for each combination of parameters, it quickly becomes a lot of different unique combinations of parameters."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a179cd6-68ca-4eef-a86c-75821f3dd0c2"}}},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder\n\nparamGrid = (ParamGridBuilder()\n  .addGrid(lr.maxIter, [1, 10, 100])\n  .addGrid(lr.fitIntercept, [True, False])\n  .addGrid(lr.standardization, [True, False])\n  .build()\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9cffd2db-1b7d-408a-bdc8-f8a1d7de0d69"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Now `paramGrid` contains all of the combinations we will test in the next step.  Take a look at what it contains."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4786c880-2525-42d4-acab-096e53f263cf"}}},{"cell_type":"code","source":["paramGrid"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"456b1818-9890-4126-bcd2-9520da65e1ce"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[5]: [{Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;maxIter&#39;, doc=&#39;max number of iterations (&gt;= 0).&#39;): 1,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;fitIntercept&#39;, doc=&#39;whether to fit an intercept term.&#39;): True,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;standardization&#39;, doc=&#39;whether to standardize the training features before fitting the model.&#39;): True},\n {Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;maxIter&#39;, doc=&#39;max number of iterations (&gt;= 0).&#39;): 1,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;fitIntercept&#39;, doc=&#39;whether to fit an intercept term.&#39;): True,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;standardization&#39;, doc=&#39;whether to standardize the training features before fitting the model.&#39;): False},\n {Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;maxIter&#39;, doc=&#39;max number of iterations (&gt;= 0).&#39;): 1,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;fitIntercept&#39;, doc=&#39;whether to fit an intercept term.&#39;): False,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;standardization&#39;, doc=&#39;whether to standardize the training features before fitting the model.&#39;): True},\n {Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;maxIter&#39;, doc=&#39;max number of iterations (&gt;= 0).&#39;): 1,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;fitIntercept&#39;, doc=&#39;whether to fit an intercept term.&#39;): False,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;standardization&#39;, doc=&#39;whether to standardize the training features before fitting the model.&#39;): False},\n {Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;maxIter&#39;, doc=&#39;max number of iterations (&gt;= 0).&#39;): 10,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;fitIntercept&#39;, doc=&#39;whether to fit an intercept term.&#39;): True,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;standardization&#39;, doc=&#39;whether to standardize the training features before fitting the model.&#39;): True},\n {Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;maxIter&#39;, doc=&#39;max number of iterations (&gt;= 0).&#39;): 10,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;fitIntercept&#39;, doc=&#39;whether to fit an intercept term.&#39;): True,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;standardization&#39;, doc=&#39;whether to standardize the training features before fitting the model.&#39;): False},\n {Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;maxIter&#39;, doc=&#39;max number of iterations (&gt;= 0).&#39;): 10,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;fitIntercept&#39;, doc=&#39;whether to fit an intercept term.&#39;): False,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;standardization&#39;, doc=&#39;whether to standardize the training features before fitting the model.&#39;): True},\n {Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;maxIter&#39;, doc=&#39;max number of iterations (&gt;= 0).&#39;): 10,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;fitIntercept&#39;, doc=&#39;whether to fit an intercept term.&#39;): False,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;standardization&#39;, doc=&#39;whether to standardize the training features before fitting the model.&#39;): False},\n {Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;maxIter&#39;, doc=&#39;max number of iterations (&gt;= 0).&#39;): 100,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;fitIntercept&#39;, doc=&#39;whether to fit an intercept term.&#39;): True,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;standardization&#39;, doc=&#39;whether to standardize the training features before fitting the model.&#39;): True},\n {Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;maxIter&#39;, doc=&#39;max number of iterations (&gt;= 0).&#39;): 100,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;fitIntercept&#39;, doc=&#39;whether to fit an intercept term.&#39;): True,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;standardization&#39;, doc=&#39;whether to standardize the training features before fitting the model.&#39;): False},\n {Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;maxIter&#39;, doc=&#39;max number of iterations (&gt;= 0).&#39;): 100,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;fitIntercept&#39;, doc=&#39;whether to fit an intercept term.&#39;): False,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;standardization&#39;, doc=&#39;whether to standardize the training features before fitting the model.&#39;): True},\n {Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;maxIter&#39;, doc=&#39;max number of iterations (&gt;= 0).&#39;): 100,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;fitIntercept&#39;, doc=&#39;whether to fit an intercept term.&#39;): False,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;standardization&#39;, doc=&#39;whether to standardize the training features before fitting the model.&#39;): False}]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[5]: [{Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;maxIter&#39;, doc=&#39;max number of iterations (&gt;= 0).&#39;): 1,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;fitIntercept&#39;, doc=&#39;whether to fit an intercept term.&#39;): True,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;standardization&#39;, doc=&#39;whether to standardize the training features before fitting the model.&#39;): True},\n {Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;maxIter&#39;, doc=&#39;max number of iterations (&gt;= 0).&#39;): 1,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;fitIntercept&#39;, doc=&#39;whether to fit an intercept term.&#39;): True,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;standardization&#39;, doc=&#39;whether to standardize the training features before fitting the model.&#39;): False},\n {Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;maxIter&#39;, doc=&#39;max number of iterations (&gt;= 0).&#39;): 1,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;fitIntercept&#39;, doc=&#39;whether to fit an intercept term.&#39;): False,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;standardization&#39;, doc=&#39;whether to standardize the training features before fitting the model.&#39;): True},\n {Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;maxIter&#39;, doc=&#39;max number of iterations (&gt;= 0).&#39;): 1,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;fitIntercept&#39;, doc=&#39;whether to fit an intercept term.&#39;): False,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;standardization&#39;, doc=&#39;whether to standardize the training features before fitting the model.&#39;): False},\n {Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;maxIter&#39;, doc=&#39;max number of iterations (&gt;= 0).&#39;): 10,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;fitIntercept&#39;, doc=&#39;whether to fit an intercept term.&#39;): True,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;standardization&#39;, doc=&#39;whether to standardize the training features before fitting the model.&#39;): True},\n {Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;maxIter&#39;, doc=&#39;max number of iterations (&gt;= 0).&#39;): 10,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;fitIntercept&#39;, doc=&#39;whether to fit an intercept term.&#39;): True,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;standardization&#39;, doc=&#39;whether to standardize the training features before fitting the model.&#39;): False},\n {Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;maxIter&#39;, doc=&#39;max number of iterations (&gt;= 0).&#39;): 10,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;fitIntercept&#39;, doc=&#39;whether to fit an intercept term.&#39;): False,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;standardization&#39;, doc=&#39;whether to standardize the training features before fitting the model.&#39;): True},\n {Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;maxIter&#39;, doc=&#39;max number of iterations (&gt;= 0).&#39;): 10,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;fitIntercept&#39;, doc=&#39;whether to fit an intercept term.&#39;): False,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;standardization&#39;, doc=&#39;whether to standardize the training features before fitting the model.&#39;): False},\n {Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;maxIter&#39;, doc=&#39;max number of iterations (&gt;= 0).&#39;): 100,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;fitIntercept&#39;, doc=&#39;whether to fit an intercept term.&#39;): True,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;standardization&#39;, doc=&#39;whether to standardize the training features before fitting the model.&#39;): True},\n {Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;maxIter&#39;, doc=&#39;max number of iterations (&gt;= 0).&#39;): 100,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;fitIntercept&#39;, doc=&#39;whether to fit an intercept term.&#39;): True,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;standardization&#39;, doc=&#39;whether to standardize the training features before fitting the model.&#39;): False},\n {Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;maxIter&#39;, doc=&#39;max number of iterations (&gt;= 0).&#39;): 100,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;fitIntercept&#39;, doc=&#39;whether to fit an intercept term.&#39;): False,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;standardization&#39;, doc=&#39;whether to standardize the training features before fitting the model.&#39;): True},\n {Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;maxIter&#39;, doc=&#39;max number of iterations (&gt;= 0).&#39;): 100,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;fitIntercept&#39;, doc=&#39;whether to fit an intercept term.&#39;): False,\n  Param(parent=&#39;LinearRegression_0c2ebf29d59f&#39;, name=&#39;standardization&#39;, doc=&#39;whether to standardize the training features before fitting the model.&#39;): False}]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n### Cross-Validation\n\nThere are a number of different ways of conducting cross-validation, allowing us to trade off between computational expense and model performance.  An exhaustive approach to cross-validation would include every possible split of the training set.  More commonly, _k_-fold cross-validation is used where the training dataset is divided into _k_ smaller sets, or folds.  A model is then trained on _k_-1 folds of the training data and the last fold is used to evaluate its performance.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> See <a href=\"https://en.wikipedia.org/wiki/Cross-validation_(statistics)\" target=\"_blank\">the Wikipedia article on Cross-Validation</a> for more information."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"03aaa4b6-67c2-4540-b2e7-7e1498dc554e"}}},{"cell_type":"markdown","source":["Create a `RegressionEvaluator()` to evaluate our grid search experiments and a `CrossValidator()` to build our models."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1611082c-b40b-4825-82a2-4f74304733a2"}}},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.tuning import CrossValidator\n\nevaluator = RegressionEvaluator(\n  labelCol = \"medv\", \n  predictionCol = \"prediction\"\n)\n\ncv = CrossValidator(\n  estimator = pipeline,             # Estimator (individual model or pipeline)\n  estimatorParamMaps = paramGrid,   # Grid of parameters to try (grid search)\n  evaluator=evaluator,              # Evaluator\n  numFolds = 3,                     # Set k to 3\n  seed = 42                         # Seed to sure our results are the same if ran again\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"379f705e-05ff-453f-96e1-b961c17ec218"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\nFit the `CrossValidator()`\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> This will train a large number of models.  If your cluster size is too small, it could take a while."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5bf09a78-3336-4523-afde-0a2df462e6e2"}}},{"cell_type":"code","source":["cvModel = cv.fit(trainDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a093ee8-620a-4986-a6b3-545bde0c6741"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Take a look at the scores from the different experiments."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"94983fa0-d4c3-45be-a33f-dd4ed22102a7"}}},{"cell_type":"code","source":["for params, score in zip(cvModel.getEstimatorParamMaps(), cvModel.avgMetrics):\n  print(\"\".join([param.name+\"\\t\"+str(params[param])+\"\\t\" for param in params]))\n  print(\"\\tScore: {}\".format(score))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"34999dc7-23df-43a7-9897-f2d9df058680"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">maxIter\t1\tfitIntercept\tTrue\tstandardization\tTrue\t\n\tScore: 4.997942926847329\nmaxIter\t1\tfitIntercept\tTrue\tstandardization\tFalse\t\n\tScore: 4.997942926847329\nmaxIter\t1\tfitIntercept\tFalse\tstandardization\tTrue\t\n\tScore: 5.331770396612241\nmaxIter\t1\tfitIntercept\tFalse\tstandardization\tFalse\t\n\tScore: 5.331770396612241\nmaxIter\t10\tfitIntercept\tTrue\tstandardization\tTrue\t\n\tScore: 4.997942926847329\nmaxIter\t10\tfitIntercept\tTrue\tstandardization\tFalse\t\n\tScore: 4.997942926847329\nmaxIter\t10\tfitIntercept\tFalse\tstandardization\tTrue\t\n\tScore: 5.331770396612241\nmaxIter\t10\tfitIntercept\tFalse\tstandardization\tFalse\t\n\tScore: 5.331770396612241\nmaxIter\t100\tfitIntercept\tTrue\tstandardization\tTrue\t\n\tScore: 4.997942926847329\nmaxIter\t100\tfitIntercept\tTrue\tstandardization\tFalse\t\n\tScore: 4.997942926847329\nmaxIter\t100\tfitIntercept\tFalse\tstandardization\tTrue\t\n\tScore: 5.331770396612241\nmaxIter\t100\tfitIntercept\tFalse\tstandardization\tFalse\t\n\tScore: 5.331770396612241\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">maxIter\t1\tfitIntercept\tTrue\tstandardization\tTrue\t\n\tScore: 4.997942926847329\nmaxIter\t1\tfitIntercept\tTrue\tstandardization\tFalse\t\n\tScore: 4.997942926847329\nmaxIter\t1\tfitIntercept\tFalse\tstandardization\tTrue\t\n\tScore: 5.331770396612241\nmaxIter\t1\tfitIntercept\tFalse\tstandardization\tFalse\t\n\tScore: 5.331770396612241\nmaxIter\t10\tfitIntercept\tTrue\tstandardization\tTrue\t\n\tScore: 4.997942926847329\nmaxIter\t10\tfitIntercept\tTrue\tstandardization\tFalse\t\n\tScore: 4.997942926847329\nmaxIter\t10\tfitIntercept\tFalse\tstandardization\tTrue\t\n\tScore: 5.331770396612241\nmaxIter\t10\tfitIntercept\tFalse\tstandardization\tFalse\t\n\tScore: 5.331770396612241\nmaxIter\t100\tfitIntercept\tTrue\tstandardization\tTrue\t\n\tScore: 4.997942926847329\nmaxIter\t100\tfitIntercept\tTrue\tstandardization\tFalse\t\n\tScore: 4.997942926847329\nmaxIter\t100\tfitIntercept\tFalse\tstandardization\tTrue\t\n\tScore: 5.331770396612241\nmaxIter\t100\tfitIntercept\tFalse\tstandardization\tFalse\t\n\tScore: 5.331770396612241\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["You can then access the best model using the `.bestModel` attribute."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f9e305e-332e-4d86-a8a3-144e2d2c0b5f"}}},{"cell_type":"code","source":["bestModel = cvModel.bestModel"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8949f76c-2333-4dac-bb29-8362eaf28ae8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Saving Models and Predictions\n\nSpark can save both the trained model we created as well as the predictions.  For online predictions such as on a stream of new data, saving the trained model and using it with Spark Streaming is a common application.  It's also common to retrain an algorithm as a nightly batch process and save the results to a database or parquet table for later use."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"884d6f8f-9fa4-4637-aee7-1b3d0a629e64"}}},{"cell_type":"markdown","source":["Save the best model."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"00dae691-6cfd-4a40-b7dc-440e910bec24"}}},{"cell_type":"code","source":["userhome = \"pinky.gtm@mail.kmutt.ac.th\"\nmodelPath = userhome + \"/cvPipelineModel\"\ndbutils.fs.rm(modelPath, recurse=True)\n\ncvModel.bestModel.save(modelPath)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fce5c28d-60e8-4d67-bb51-f4ab721defb3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Take a look at where it saved."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5cef7c6f-218e-46e2-bd69-54959c61c2a9"}}},{"cell_type":"code","source":["dbutils.fs.ls(modelPath)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"00ac3ffa-3769-4711-853e-44efa3af3018"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[12]: [FileInfo(path=&#39;dbfs:/pinky.gtm@mail.kmutt.ac.th/cvPipelineModel/metadata/&#39;, name=&#39;metadata/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/pinky.gtm@mail.kmutt.ac.th/cvPipelineModel/stages/&#39;, name=&#39;stages/&#39;, size=0)]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[12]: [FileInfo(path=&#39;dbfs:/pinky.gtm@mail.kmutt.ac.th/cvPipelineModel/metadata/&#39;, name=&#39;metadata/&#39;, size=0),\n FileInfo(path=&#39;dbfs:/pinky.gtm@mail.kmutt.ac.th/cvPipelineModel/stages/&#39;, name=&#39;stages/&#39;, size=0)]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Save predictions made on `testDF`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b13217f-ae96-47eb-8d29-844c9d093c97"}}},{"cell_type":"code","source":["predictionsPath = userhome + \"/modelPredictions.parquet\"\n\ncvModel.bestModel.transform(testDF).write.mode(\"OVERWRITE\").parquet(predictionsPath)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cecf17b6-7512-4f09-a9db-98739db541ed"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"1. Model Selection","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1213335670089644}},"nbformat":4,"nbformat_minor":0}
